{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Perceptron\n",
    "Below code reprsents working of a perceptron (with 1 Nueron) using **Gradient Descent Algorithm**.\n",
    "\n",
    "But will apply a sigmoid activation function at out neuron (Green Circle)\n",
    "<img src=\"Basic_Preceptron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sigmoid function\n",
    "\n",
    "$g(z) = \\frac{1}{1+{e}^{-z}}$\n",
    "\n",
    "### Derivative of sigmoid function\n",
    "\n",
    "$g^{\\prime}(z) = g(z)(1-g(z)$\n",
    "\n",
    "<img src=\"sigmoid.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define activation function sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# define derivative of sigmoid function\n",
    "def sigmoid_deriv(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Explained (Maths) using sigmoid activation function\n",
    "\n",
    "$\\hat{y} = sigmoid(w_1x_1 + w_2x_2) $\n",
    "\n",
    "### Cost\n",
    "\n",
    "$C = \\frac{1}{2}(y - \\hat{y})^2$\n",
    "\n",
    "$C = \\frac{1}{2}(y - sigmoid(w_1x_1 - w_2x_2))^2$\n",
    "\n",
    "### Getting Gradients\n",
    "$\\frac{dC}{dw_1} = \\frac{1}{2}*2(y - sigmoid(w_1x_1 - w_2x_2))\\frac{d\\,sigmoid(w_1x_1 - w_2x_2)}{dw_1} $\n",
    "\n",
    "$\\frac{dC}{dw_1} = \\frac{1}{2}*2(y - \\hat{y})\\frac{d\\,\\hat{y}}{dw_1}$\n",
    "\n",
    "$\\frac{dC}{dw_1} = \\frac{1}{2}*2(y - \\hat{y})(\\hat{y})(1 - \\hat{y})(-x_1)$\n",
    "\n",
    "similarly \n",
    "\n",
    "$\\frac{dC}{dw_2} = \\frac{1}{2}*2(y - \\hat{y})(\\hat{y})(1 - \\hat{y})(-x_2)$\n",
    "### Updating weights\n",
    "$w_1 \\to w_1 - lr*\\frac{dC}{dw_1} $  where : lr is learning rate or step size\n",
    "\n",
    "$w_2 \\to w_2 - lr*\\frac{dC}{dw_2}$\n",
    "\n",
    "### Calculate new $\\hat{y}$ using new weights\n",
    "$\\hat{y} = sigmoid(w_1x_1 + w_2x_2) $\n",
    "\n",
    "**Repeat the above process till we reach global minima of cost function i.e. minimum cost.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Perceptron using Sigmoid Activation Function\n",
    "\n",
    "Below model is same as we did in previous model exercise but we apply sigmoid activation function on neuron and while doing gradient descent we use the derivation of sigmoid (explained above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_batch_generalized(x,y,w,lr,n_epoch):\n",
    "    cost_list = []  # initialize list to store epochs and cost\n",
    "    y_hat = sigmoid(np.dot(x,w))  # calculate y_hat for first iteration note activation function sigmoid applied\n",
    "    for epoch in range(n_epoch):\n",
    "        dcostdw = 2*0.5*np.dot(-x.T,np.multiply((y-y_hat),sigmoid_deriv(y_hat)))\n",
    "        w = w - lr*dcostdw\n",
    "        y_hat = sigmoid(np.dot(x,w))  # new y_hat using updated weights\n",
    "        cost = 0.5*np.sum(y - y_hat)**2\n",
    "        cost_list.append([epoch,cost])\n",
    "        if epoch%100==0:\n",
    "            print(\"epoch :{:d} cost:{:f}\".format(epoch,cost))\n",
    "        if cost <= 0.001:\n",
    "            print(\"epoch :{:d} cost:{:f}\".format(epoch,cost))\n",
    "            break\n",
    "\n",
    "    cost_list = pd.DataFrame(cost_list,columns=['epoch','cost'])\n",
    "    return w,cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try using different values in x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :0 cost:0.337389\n",
      "epoch :51 cost:0.000035\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[10,10,20],\n",
    "              [8,24,25],\n",
    "              [30,5,23],\n",
    "              [18,25,28],\n",
    "              [5,2,9]])\n",
    "\n",
    "y = [[0],[0],[1],[1],[0]]\n",
    "w = np.zeros((x.shape[1],1))\n",
    "lr = 0.01\n",
    "n_epoch = 10000\n",
    "w,cost_list = nn_batch_generalized(x,y,w,lr,n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat_final:\n",
      "[[ 0.14114532]\n",
      " [ 0.10988658]\n",
      " [ 0.93456342]\n",
      " [ 0.53576365]\n",
      " [ 0.27025969]]\n",
      "trained weights :\n",
      "[[ 0.30264395]\n",
      " [ 0.1328052 ]\n",
      " [-0.30801506]]\n"
     ]
    }
   ],
   "source": [
    "print \"y_hat_final:\\n{:s}\".format(sigmoid(np.dot(x,w)))\n",
    "print \"trained weights :\\n{:s}\".format(w)\n",
    "# values >=0.5 represent 1 else 0 # refer sigmoid plot above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
